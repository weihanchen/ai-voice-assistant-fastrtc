"""FastRTC ReplyOnPause è™•ç†å™¨æ•´åˆ

å»ºç«‹ FastRTC èªéŸ³ä¸²æµï¼Œé…ç½® ReplyOnPause æ©Ÿåˆ¶ã€‚
"""

import logging

import gradio as gr
import numpy as np
from fastrtc import AlgoOptions, ReplyOnPause, SileroVadOptions, Stream, WebRTC

from voice_assistant.config import Settings
from voice_assistant.llm.client import LLMClient
from voice_assistant.roles.predefined.assistant import AssistantRole
from voice_assistant.roles.predefined.coach import CoachRole
from voice_assistant.roles.predefined.interviewer import InterviewerRole
from voice_assistant.roles.registry import RoleRegistry
from voice_assistant.tools import (
    ExchangeRateTool,
    StockPriceTool,
    ToolRegistry,
    WeatherTool,
)
from voice_assistant.voice.pipeline import VoicePipeline
from voice_assistant.voice.schemas import VoicePipelineConfig
from voice_assistant.voice.ui import (
    additional_outputs_handler,
    audio_input_handler,
    create_additional_outputs,
)

logger = logging.getLogger(__name__)


def create_voice_stream(settings: Settings) -> Stream:
    """å»ºç«‹ FastRTC èªéŸ³ä¸²æµ

    Args:
        settings: æ‡‰ç”¨ç¨‹å¼è¨­å®š

    Returns:
        é…ç½®å¥½çš„ FastRTC Streamï¼ˆå·²è¨­å®šè‡ªå®šç¾© UI èˆ‡äº‹ä»¶ç¶å®šï¼‰
    """
    # åˆå§‹åŒ– LLM Client
    llm_client = LLMClient(
        api_key=settings.openai_api_key,
        model=settings.openai_model,
    )

    # å»ºç«‹èªéŸ³ç®¡ç·šé…ç½®ï¼ˆä½¿ç”¨æ­£ç¢º config é¡åˆ¥ï¼‰
    from voice_assistant.voice.schemas import (
        STTConfig,
        TTSConfig,
        VADConfig,
    )

    config = VoicePipelineConfig(
        stt=STTConfig(
            model_size=settings.whisper_model_size,
            model_path=settings.whisper_model_path,
            language=settings.whisper_language,
            device=settings.whisper_device,
        ),
        tts=TTSConfig(
            model_path=settings.tts_model_path,
            voice=settings.tts_voice,
            speed=settings.tts_speed,
        ),
        vad=VADConfig(
            pause_threshold_ms=settings.vad_pause_threshold_ms,
            min_speech_duration_ms=settings.vad_min_speech_duration_ms,
            speech_threshold=settings.vad_speech_threshold,
            min_silence_duration_ms=settings.vad_min_silence_duration_ms,
        ),
        can_interrupt=True,
        server_host=settings.server_host,
        server_port=settings.server_port,
    )

    # åˆå§‹åŒ–å·¥å…·è¨»å†Šè¡¨ï¼ˆComposition Rootï¼‰
    tool_registry = ToolRegistry()
    tool_registry.register(WeatherTool())
    tool_registry.register(ExchangeRateTool())
    tool_registry.register(StockPriceTool())

    # ----------
    # åˆå§‹åŒ–è§’è‰²è¨»å†Šè¡¨èˆ‡é è¨­è§’è‰²
    role_registry = RoleRegistry()
    role_registry.register(AssistantRole())
    role_registry.register(CoachRole())
    role_registry.register(InterviewerRole())

    # å»ºç«‹è§’è‰²é¸é …ï¼šID -> é¡¯ç¤ºåç¨±ï¼ˆå« emojiï¼‰
    role_display_map = {
        "assistant": "ğŸ¤– åŠ©ç†",
        "coach": "ğŸ’ª æ•™ç·´",
        "interviewer": "ğŸ‘” é¢è©¦å®˜",
    }
    available_roles = {
        role.id: role_display_map.get(role.id, role.name)
        for role in role_registry.list_roles()
    }
    default_role_id = next(iter(available_roles)) if available_roles else ""

    # åˆå§‹åŒ–æ„åœ–è¾¨è­˜å™¨
    from voice_assistant.intent.recognizer import IntentRecognizer

    intent_recognizer = IntentRecognizer(llm_client)

    # åˆå§‹åŒ–èªéŸ³ç®¡ç·šï¼ˆä½¿ç”¨æ­£ç¢ºçš„ 007 + 008 æ•´åˆç‰ˆæœ¬ï¼‰
    pipeline = VoicePipeline(
        config=config,
        llm_client=llm_client,
        tool_registry=tool_registry,
        intent_recognizer=intent_recognizer,
        role_registry=role_registry,
    )
    # å•Ÿå‹•éšæ®µå…ˆè¨­ç½®é è¨­è§’è‰²
    if default_role_id:
        pipeline.switch_role(role_registry.get(default_role_id))

    # å›èª¿ glueï¼šè§’è‰²åˆ‡æ›
    def on_role_change(role_id: str, current_chatbot: list, current_status: str):
        # é˜²ç¦¦å¼ï¼šç¢ºä¿ current_chatbot å’Œ current_status æœ‰é è¨­å€¼
        current_chatbot = current_chatbot or []
        current_status = current_status or "ğŸŸ¢ å¾…å‘½"
        role = role_registry.get(role_id)
        pipeline.switch_role(role)
        welcome = (
            role.get_welcome_message() if hasattr(role, "get_welcome_message") else None
        )
        if welcome:
            # åœ¨æ­¡è¿èªå‰åŠ å…¥è¦–è¦ºåˆ†éš”ï¼Œé¿å…èˆ‡ä¸Šä¸€å¥ç·Šé„°
            updated_chatbot = current_chatbot + [
                {"role": "assistant", "content": f"---\n\n{welcome}"}
            ]
            updated_status = f"ğŸŸ¢ {role.name}å·²å•Ÿç”¨"
            # æ³¨æ„ï¼šæ­¡è¿èªåªåœ¨å°è©±æ¡†é¡¯ç¤ºï¼Œä¸æ’­æ”¾ TTSï¼ˆé¿å…èˆ‡ WebRTC ä¸²æµè¡çªï¼‰
            return updated_chatbot, updated_status
        return current_chatbot, current_status

    # å»ºç«‹é¡å¤–è¼¸å‡ºå…ƒä»¶ï¼ˆChatbot å’Œç‹€æ…‹ï¼‰
    chatbot, status_display = create_additional_outputs()

    # ---- [AI assistant injects welcome on initial load] ----
    initial_history = []
    initial_status = "ğŸŸ¢ å¾…å‘½"
    default_role = role_registry.get(default_role_id) if default_role_id else None

    if default_role and hasattr(default_role, "get_welcome_message"):
        welcome_msg = default_role.get_welcome_message()
        if welcome_msg:
            initial_history = [{"role": "assistant", "content": welcome_msg}]
            initial_status = f"ğŸŸ¢ {default_role.name}å·²å•Ÿç”¨"
            # æ³¨æ„ï¼šé¦–æ¬¡è¼‰å…¥æ™‚ä¸æ’­æ”¾ TTSï¼ˆWebRTC é€£ç·šå°šæœªå»ºç«‹ï¼‰

    chatbot.value = initial_history
    status_display.value = initial_status

    # å»ºç«‹ FastRTC Streamï¼ˆä½¿ç”¨ process_audio_with_outputs ä»¥æ”¯æ´ AdditionalOutputsï¼‰
    stream = Stream(
        handler=ReplyOnPause(
            pipeline.process_audio_with_outputs,
            algo_options=AlgoOptions(
                audio_chunk_duration=config.vad.pause_threshold_ms / 1000,
                started_talking_threshold=0.2,
                speech_threshold=config.vad.speech_threshold,
            ),
            model_options=SileroVadOptions(
                threshold=0.5,
                min_speech_duration_ms=config.vad.min_speech_duration_ms,
                min_silence_duration_ms=config.vad.pause_threshold_ms,
            ),
            can_interrupt=config.can_interrupt,
            output_sample_rate=config.tts.sample_rate,
        ),
        modality="audio",
        mode="send-receive",
        additional_outputs=[chatbot, status_display],
        additional_outputs_handler=additional_outputs_handler,
    )

    # å»ºç«‹æ¸…é™¤å°è©±çš„å‡½å¼ï¼ˆé–‰åŒ…ï¼Œæ•ç² pipeline åƒè€ƒï¼‰
    def clear_conversation() -> tuple[list[dict[str, str]], str]:
        """æ¸…é™¤å°è©±æ­·å²

        åŒæ™‚æ¸…é™¤ UI é¡¯ç¤ºå’Œ pipeline å…§éƒ¨ç‹€æ…‹ã€‚

        Returns:
            (empty_chatbot, reset_status)
        """
        pipeline.state.history.clear()
        logger.info("[Handler] å°è©±æ­·å²å·²æ¸…é™¤")
        return [], "ğŸŸ¢ å¾…å‘½"

    # å»ºç«‹è™•ç†ä¸Šå‚³éŸ³è¨Šçš„å‡½å¼ï¼ˆé–‰åŒ…ï¼Œæ•ç² pipeline åƒè€ƒï¼‰
    def process_uploaded_audio(
        audio: tuple[int, np.ndarray] | None,
        current_chatbot: list[dict[str, str]],
        current_status: str,
    ) -> tuple[list[dict[str, str]], str, None]:
        """è™•ç†ä¸Šå‚³çš„éŸ³è¨Šæª”æ¡ˆ

        Args:
            audio: (sample_rate, audio_array) æˆ– None
            current_chatbot: ç›®å‰çš„å°è©±è¨˜éŒ„
            current_status: ç›®å‰çš„ç‹€æ…‹æ–‡å­—

        Returns:
            (updated_chatbot, updated_status, cleared_audio_input)
        """
        if audio is None:
            return current_chatbot, current_status, None

        # è½‰æ›éŸ³è¨Šæ ¼å¼
        processed_audio = audio_input_handler(audio)
        if processed_audio is None:
            return current_chatbot, current_status, None

        logger.info("[Handler] é–‹å§‹è™•ç†ä¸Šå‚³çš„éŸ³è¨Šæª”æ¡ˆ")

        # ä½¿ç”¨ pipeline è™•ç†éŸ³è¨Šï¼ˆåŒæ­¥æ–¹å¼ï¼‰
        # æ”¶é›†æ‰€æœ‰è¼¸å‡º
        final_chatbot = current_chatbot
        final_status = current_status

        try:
            for output in pipeline.process_audio_with_outputs(processed_audio):
                # æª¢æŸ¥æ˜¯å¦ç‚º AdditionalOutputs
                from fastrtc import AdditionalOutputs

                if isinstance(output, AdditionalOutputs):
                    # AdditionalOutputs ç‰©ä»¶
                    final_chatbot = output.args[0]
                    final_status = output.args[1]
                # éŸ³è¨Šè¼¸å‡ºåœ¨é€™è£¡å¿½ç•¥ï¼ˆä¸æ’­æ”¾ TTSï¼‰
        except Exception as e:
            logger.error(f"[Handler] è™•ç†ä¸Šå‚³éŸ³è¨Šå¤±æ•—: {e}", exc_info=True)
            final_status = f"âŒ è™•ç†å¤±æ•—: {e}"

        logger.info("[Handler] ä¸Šå‚³éŸ³è¨Šè™•ç†å®Œæˆ")
        return final_chatbot, final_status, None

    # å»ºç«‹è‡ªè¨‚ UIï¼Œæ·»åŠ éŸ³è¨Šä¸Šå‚³åŠŸèƒ½
    sidebar_css = """
    .gradio-container .sidebar {
        background-color: color-mix(
            in srgb, var(--block-background-fill) 50%, transparent
        ) !important;
    }
    body.dark .gradio-container .sidebar {
        background-color: color-mix(
            in srgb, var(--block-background-fill) 50%, transparent
        ) !important;
    }
    """
    with gr.Blocks(title="AI èªéŸ³åŠ©ç†", css=sidebar_css) as custom_ui:
        gr.HTML("<h1 style='text-align: center'>AI èªéŸ³åŠ©ç†</h1>")

        with gr.Row():
            # å·¦å´ï¼šå°è©±è¨˜éŒ„ï¼ˆä¸»è¦å€åŸŸï¼‰
            with gr.Column(scale=2):
                chatbot.render()
                clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤å°è©±", variant="secondary")

            # å³å´ï¼šæ§åˆ¶å€
            with gr.Column(scale=1):
                # è§’è‰²åˆ‡æ›ä¸‹æ‹‰é¸å–®å…ƒä»¶
                # choices æ ¼å¼ï¼š[(é¡¯ç¤ºåç¨±, role_id), ...]
                dropdown = gr.Dropdown(
                    choices=[
                        (display_name, role_id)
                        for role_id, display_name in available_roles.items()
                    ],
                    value=default_role_id,
                    label="é¸æ“‡è§’è‰²",
                    interactive=True,
                )
                # æ­£ç¢ºç¶å®š chatbot/state åšåˆ° UI æ›´æ–°
                dropdown.change(
                    fn=on_role_change,
                    inputs=[dropdown, chatbot, status_display],
                    outputs=[chatbot, status_display],
                )

                # WebRTC ä¸²æµå…ƒä»¶ï¼ˆæ”¾åœ¨å³å´ä¸Šæ–¹ï¼Œé—œé–‰å…¨è¢å¹•æ¨¡å¼ï¼‰
                webrtc = WebRTC(
                    label="èªéŸ³ä¸²æµ",
                    mode="send-receive",
                    modality="audio",
                    full_screen=False,
                )
                stream.webrtc_component = webrtc

                status_display.render()

                # éŸ³è¨Šä¸Šå‚³å€
                with gr.Accordion("ğŸ“ éŸ³è¨Šæª”æ¡ˆæ¸¬è©¦æ¨¡å¼", open=False):
                    gr.Markdown(
                        "ä¸Šå‚³é éŒ„çš„éŸ³è¨Šæª”æ¡ˆä¾†æ¸¬è©¦å°è©±åŠŸèƒ½ï¼Œé©ç”¨æ–¼ç„¡æ³•ä½¿ç”¨éº¥å…‹é¢¨çš„ç’°å¢ƒã€‚"
                    )
                    audio_input = gr.Audio(
                        label="ä¸Šå‚³éŸ³è¨Šæª”æ¡ˆ",
                        type="numpy",
                        sources=["upload"],
                    )
                    submit_btn = gr.Button("ğŸ¯ è™•ç†éŸ³è¨Š", variant="primary")

        # ç¶å®š WebRTC ä¸²æµäº‹ä»¶
        webrtc.stream(
            fn=stream.event_handler,
            inputs=[webrtc],
            outputs=[webrtc],
            time_limit=stream.time_limit,
            concurrency_limit=stream.concurrency_limit,
        )

        # ç¶å®š AdditionalOutputs äº‹ä»¶
        webrtc.on_additional_outputs(
            additional_outputs_handler,
            inputs=[chatbot, status_display],
            outputs=[chatbot, status_display],
            concurrency_limit=stream.concurrency_limit,
        )

        # ç¶å®šéŸ³è¨Šä¸Šå‚³è™•ç†äº‹ä»¶
        submit_btn.click(
            fn=process_uploaded_audio,
            inputs=[audio_input, chatbot, status_display],
            outputs=[chatbot, status_display, audio_input],
        )

        # ç¶å®šæ¸…é™¤å°è©±äº‹ä»¶
        clear_btn.click(
            fn=clear_conversation,
            inputs=[],
            outputs=[chatbot, status_display],
        )

    # æ›¿æ› Stream çš„é è¨­ UI
    stream.ui = custom_ui

    return stream
